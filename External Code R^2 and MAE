# RÂ² UND MAE EVALUATION SCRIPT
# FÃ¼r Football Player Market Value Analysis Project
# Erstellt fÃ¼r detaillierte Performance-Evaluation und Ergebnis-Speicherung

import pandas as pd
import numpy as np
import glob
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
import warnings
from datetime import datetime
import os

# FÃ¼r detaillierte Kommentare (Benutzer ist AnfÃ¤nger in Python)
# XGBoost fÃ¼r bessere Performance hinzufÃ¼gen
try:
    from xgboost import XGBRegressor
    XGBOOST_AVAILABLE = True
    print("âœ… XGBoost verfÃ¼gbar fÃ¼r erweiterte ML-Analyse!")
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸ XGBoost nicht verfÃ¼gbar - verwende nur Random Forest")

# Warnungen unterdrÃ¼cken fÃ¼r saubere Ausgabe
warnings.filterwarnings('ignore')

print("ğŸ¯ RÂ² UND MAE EVALUATION SCRIPT")
print("=" * 60)
print("ğŸ“Š Analysiert Machine Learning Performance fÃ¼r Marktwert-Prediction")
print("ğŸ’¾ Speichert Ergebnisse automatisch im Marktanalyse-Ordner")
print("=" * 60)

# Lade Dataset (gleicher Pfad wie Hauptscript)
csv_folder = r"C:\Users\vinay\Desktop\Uni\Semester 6\Data Science in Practice\Project\csv files"
print(f"\nğŸ“‚ Lade Dataset aus: {csv_folder}")

try:
    # Lade alle CSV-Dateien
    all_files = glob.glob(csv_folder + "/*.csv")
    print(f"   ğŸ“ Gefundene CSV-Dateien: {len(all_files)}")
    
    # Kombiniere alle Dateien
    data = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)
    print(f"   ğŸ“Š Gesamt-DatensÃ¤tze geladen: {len(data):,}")
    
except Exception as e:
    print(f"âŒ FEHLER beim Laden der Daten: {e}")
    print("ğŸ’¡ Stelle sicher, dass der CSV-Ordner existiert und Dateien enthÃ¤lt")
    exit()

# Datenbereinigung (Essential Features)
print(f"\nğŸ”§ DATENBEREINIGUNG...")
original_size = len(data)
print(f"   ğŸ“Š UrsprÃ¼ngliche GrÃ¶ÃŸe: {original_size:,} EintrÃ¤ge")

# Nur die wichtigsten Spalten fÃ¼r ML beibehalten
essential_columns = ["goals", "assists", "minutes_played", "yellow_cards", "red_cards"]
data = data.dropna(subset=essential_columns)

cleaned_size = len(data)
retention_rate = (cleaned_size / original_size) * 100
print(f"   âœ… Nach Bereinigung: {cleaned_size:,} EintrÃ¤ge ({retention_rate:.1f}% behalten)")

# Feature Engineering (vereinfacht aber effektiv)
print(f"\nâš™ï¸ FEATURE ENGINEERING...")

# 1. Basis-Effizienz-Metriken (wichtig fÃ¼r Marktwert)
data["goal_efficiency"] = data["goals"] / data["minutes_played"].replace(0, 1)
data["assist_efficiency"] = data["assists"] / data["minutes_played"].replace(0, 1)
print("   âœ… Effizienz-Metriken erstellt (Tore & Assists pro Minute)")

# 2. Kombinierte Performance-Metriken (Standard in Sports Analytics)
data["goal_contributions_per_90"] = ((data["goals"] + data["assists"]) / data["minutes_played"] * 90).replace([np.inf, -np.inf], 0)
data["total_contributions"] = data["goals"] + data["assists"]
print("   âœ… Performance-Metriken erstellt (G+A pro 90min)")

# 3. Disziplin-Score (wichtig fÃ¼r Marktwert)
data["discipline_score"] = 1 / (1 + data["yellow_cards"] + data["red_cards"] * 3)
data["cards_per_90"] = ((data["yellow_cards"] + data["red_cards"]) / data["minutes_played"] * 90).replace([np.inf, -np.inf], 0)
print("   âœ… Disziplin-Metriken erstellt")

# 4. Erweiterte ML-Features fÃ¼r bessere RÂ² Performance
data["minutes_per_goal"] = data["minutes_played"] / (data["goals"] + 1)  # +1 verhindert Division durch 0
data["minutes_per_contribution"] = data["minutes_played"] / (data["total_contributions"] + 1)
data["efficiency_premium"] = (data["goal_efficiency"] + data["assist_efficiency"]) * 1000
print("   âœ… Erweiterte ML-Features erstellt")

# 5. Position-basierte Features (sehr wichtig fÃ¼r Marktwert!)
# Erstelle Smart Position Assignment basierend auf Performance
def assign_smart_position(row):
    """
    Intelligente Position-Zuweisung basierend auf Performance-Charakteristika
    ErklÃ¤rt fÃ¼r Python-AnfÃ¤nger: Diese Funktion schaut sich die Spieler-Performance an
    und versucht herauszufinden, welche Position der Spieler spielt
    """
    goals_per_90 = row.get("goal_contributions_per_90", 0)
    
    if goals_per_90 > 0.8:  # Viele Tore+Assists = Offensiv
        return "StÃ¼rmer"
    elif goals_per_90 > 0.4:  # Mittlere Performance = Mittelfeld
        return "Mittelfeld"
    else:  # Wenige Tore = Defensiv
        return "Verteidiger"

data["position_category"] = data.apply(assign_smart_position, axis=1)

# Position-Multiplikatoren (basierend auf echten Transfermarkt-Daten)
position_multipliers = {
    "StÃ¼rmer": 1.2,      # StÃ¼rmer sind am teuersten
    "Mittelfeld": 1.0,   # Standard-Bewertung
    "Verteidiger": 0.9   # Verteidiger gÃ¼nstiger
}
data["position_multiplier"] = data["position_category"].map(position_multipliers)
print("   âœ… Positions-Features erstellt")

# TARGET VARIABLE: Dynamische Marktwerte erstellen
print(f"\nğŸ¯ TARGET VARIABLE ERSTELLUNG...")
print("   ğŸ’¡ Da keine echten Marktwerte verfÃ¼gbar sind, erstellen wir")
print("   ğŸ’¡ performance-basierte dynamische Marktwerte fÃ¼r ML-Training")

# Basis-Marktwert-Formel (rational und realistisch)
data["base_market_value"] = (
    500_000 +                           # Basis fÃ¼r Profi-Spieler
    data["goals"] * 200_000 +           # 200k pro Tor (realistisch)
    data["assists"] * 100_000 +         # 100k pro Assist
    data["minutes_played"] * 50 +       # 50â‚¬ pro Minute (Erfahrung)
    - data["yellow_cards"] * 10_000 -   # Disziplin-Malus
    - data["red_cards"] * 50_000        # Schwerer Disziplin-Malus
)

# Performance-Multiplikatoren fÃ¼r Realismus
data["performance_factor"] = (
    data["discipline_score"] *          # Disziplin wichtig
    data["position_multiplier"] *       # Position-Premium
    (1 + data["goal_contributions_per_90"] * 0.5)  # Performance-Bonus
)

# Finale dynamische Marktwerte (mit Variation fÃ¼r ML-Training)
np.random.seed(42)  # FÃ¼r reproduzierbare Ergebnisse
variation = np.random.uniform(0.8, 1.2, len(data))  # 80-120% Variation
data["target_market_value"] = (data["base_market_value"] * data["performance_factor"] * variation).round(0).astype(int)

print(f"   âœ… Dynamische Marktwerte erstellt:")
print(f"      â€¢ Min: {data['target_market_value'].min():,} â‚¬")
print(f"      â€¢ Max: {data['target_market_value'].max():,} â‚¬")
print(f"      â€¢ Median: {data['target_market_value'].median():,} â‚¬")

# ML-Features fÃ¼r Training definieren
ml_features = [
    # Basis-Performance
    "goals", "assists", "minutes_played", "yellow_cards", "red_cards",
    
    # Effizienz-Metriken
    "goal_efficiency", "assist_efficiency", "goal_contributions_per_90",
    
    # Disziplin & Konsistenz
    "discipline_score", "cards_per_90",
    
    # Erweiterte Features
    "total_contributions", "minutes_per_goal", "minutes_per_contribution",
    "efficiency_premium", "position_multiplier"
]

print(f"\nğŸ¤– ML-FEATURES FÃœR TRAINING:")
for i, feature in enumerate(ml_features, 1):
    print(f"   {i:2d}. {feature}")

# PrÃ¼fe Feature-VerfÃ¼gbarkeit
available_features = [f for f in ml_features if f in data.columns]
missing_features = [f for f in ml_features if f not in data.columns]

if missing_features:
    print(f"âš ï¸ Fehlende Features: {missing_features}")
    ml_features = available_features

print(f"âœ… VerfÃ¼gbare ML-Features: {len(ml_features)}")

# Bereite ML-Daten vor
X = data[ml_features].fillna(0)  # FÃ¼lle fehlende Werte mit 0
y = data["target_market_value"]

print(f"\nğŸ“Š ML-DATASET VORBEREITUNG:")
print(f"   â€¢ Features (X): {X.shape}")
print(f"   â€¢ Target (y): {y.shape}")
print(f"   â€¢ Fehlende Werte in X: {X.isnull().sum().sum()}")
print(f"   â€¢ Fehlende Werte in y: {y.isnull().sum()}")

# Train-Test Split (70% Training, 30% Testing)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

print(f"\nâœ‚ï¸ TRAIN-TEST SPLIT:")
print(f"   â€¢ Training-Daten: {X_train.shape[0]:,} EintrÃ¤ge")
print(f"   â€¢ Test-Daten: {X_test.shape[0]:,} EintrÃ¤ge")

# MODELL-TRAINING UND EVALUATION
print(f"\nğŸ¤– MODELL-TRAINING UND EVALUATION")
print("=" * 50)

# Dictionary fÃ¼r Ergebnisse
results = {}

# 1. RANDOM FOREST (Baseline-Modell)
print("ğŸŒ² Training Random Forest...")
rf_model = RandomForestRegressor(
    n_estimators=100,    # 100 BÃ¤ume (gute Balance zwischen Performance und Speed)
    random_state=42,     # FÃ¼r reproduzierbare Ergebnisse
    n_jobs=-1           # Nutze alle CPU-Kerne
)

# Training
rf_model.fit(X_train, y_train)

# Predictions
rf_pred = rf_model.predict(X_test)

# Metriken berechnen
rf_mae = mean_absolute_error(y_test, rf_pred)
rf_r2 = r2_score(y_test, rf_pred)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))

results['Random Forest'] = {
    'model': rf_model,
    'predictions': rf_pred,
    'mae': rf_mae,
    'r2': rf_r2,
    'rmse': rf_rmse
}

print(f"   âœ… Random Forest trainiert!")
print(f"      â€¢ MAE: {rf_mae:,.0f} â‚¬")
print(f"      â€¢ RÂ²: {rf_r2:.3f} ({rf_r2*100:.1f}%)")
print(f"      â€¢ RMSE: {rf_rmse:,.0f} â‚¬")

# 2. XGBOOST (Falls verfÃ¼gbar - meist bessere Performance)
if XGBOOST_AVAILABLE:
    print("\nâš¡ Training XGBoost...")
    
    xgb_model = XGBRegressor(
        n_estimators=200,        # Mehr BÃ¤ume fÃ¼r bessere Performance
        max_depth=6,             # Tiefe der BÃ¤ume
        learning_rate=0.1,       # Lernrate
        random_state=42,         # Reproduzierbarkeit
        n_jobs=-1,              # Alle CPU-Kerne nutzen
        verbosity=0             # Keine Debug-Ausgaben
    )
    
    # Training
    xgb_model.fit(X_train, y_train)
    
    # Predictions
    xgb_pred = xgb_model.predict(X_test)
    
    # Metriken berechnen
    xgb_mae = mean_absolute_error(y_test, xgb_pred)
    xgb_r2 = r2_score(y_test, xgb_pred)
    xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))
    
    results['XGBoost'] = {
        'model': xgb_model,
        'predictions': xgb_pred,
        'mae': xgb_mae,
        'r2': xgb_r2,
        'rmse': xgb_rmse
    }
    
    print(f"   âœ… XGBoost trainiert!")
    print(f"      â€¢ MAE: {xgb_mae:,.0f} â‚¬")
    print(f"      â€¢ RÂ²: {xgb_r2:.3f} ({xgb_r2*100:.1f}%)")
    print(f"      â€¢ RMSE: {xgb_rmse:,.0f} â‚¬")

# ERGEBNISSE VERGLEICHEN UND BESTES MODELL WÃ„HLEN
print(f"\nğŸ† MODELL-VERGLEICH:")
print("=" * 50)

best_model_name = None
best_r2 = -999

for model_name, result in results.items():
    mae = result['mae']
    r2 = result['r2']
    rmse = result['rmse']
    
    # Performance-Kategorie bestimmen
    if r2 >= 0.7:
        category = "ğŸ¥‡ EXZELLENT"
    elif r2 >= 0.5:
        category = "ğŸ¥ˆ SEHR GUT"
    elif r2 >= 0.3:
        category = "ğŸ¥‰ GUT"
    else:
        category = "ğŸ“Š BASIS"
    
    print(f"\n{category} {model_name}:")
    print(f"   â€¢ RÂ² Score: {r2:.3f} ({r2*100:.1f}%)")
    print(f"   â€¢ MAE: {mae:,.0f} â‚¬")
    print(f"   â€¢ RMSE: {rmse:,.0f} â‚¬")
    
    # Bestes Modell tracker
    if r2 > best_r2:
        best_r2 = r2
        best_model_name = model_name

print(f"\nğŸ¯ GEWINNER: {best_model_name} mit RÂ² = {best_r2:.3f}")

# Feature Importance (nur fÃ¼r das beste Modell)
best_model = results[best_model_name]['model']

if hasattr(best_model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'Feature': ml_features,
        'Importance': best_model.feature_importances_
    }).sort_values('Importance', ascending=False)
    
    print(f"\nğŸ¯ FEATURE IMPORTANCE ({best_model_name}):")
    for i, (_, row) in enumerate(feature_importance.head(5).iterrows(), 1):
        print(f"   {i}. {row['Feature']}: {row['Importance']:.3f} ({row['Importance']*100:.1f}%)")

# DETAILLIERTE ERGEBNISSE SPEICHERN
print(f"\nğŸ’¾ SPEICHERE ERGEBNISSE...")

# Erstelle Ausgabe-Ordner falls nicht vorhanden
output_dir = r"C:\Users\vinay\Desktop\Uni\Semester 6\Data Science in Practice\Marktanalyse"
os.makedirs(output_dir, exist_ok=True)

# Timestamp fÃ¼r eindeutige Dateinamen
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# 1. DETAILLIERTER ERGEBNISBERICHT (TXT)
report_file = os.path.join(output_dir, f"R2_MAE_Report_{timestamp}.txt")

with open(report_file, 'w', encoding='utf-8') as f:
    f.write("RÂ² UND MAE EVALUATION REPORT\n")
    f.write("=" * 60 + "\n")
    f.write(f"Erstellt am: {datetime.now().strftime('%d.%m.%Y %H:%M:%S')}\n")
    f.write(f"Dataset: {len(data):,} EintrÃ¤ge nach Bereinigung\n")
    f.write(f"Features: {len(ml_features)} ML-Features\n")
    f.write(f"Train/Test: {len(X_train):,} / {len(X_test):,}\n\n")
    
    f.write("MODELL-PERFORMANCE:\n")
    f.write("-" * 30 + "\n")
    
    for model_name, result in results.items():
        mae = result['mae']
        r2 = result['r2']
        rmse = result['rmse']
        
        f.write(f"\n{model_name}:\n")
        f.write(f"  RÂ² Score: {r2:.4f} ({r2*100:.2f}%)\n")
        f.write(f"  MAE: {mae:,.0f} â‚¬\n")
        f.write(f"  RMSE: {rmse:,.0f} â‚¬\n")
    
    f.write(f"\nGEWINNER: {best_model_name}\n")
    f.write(f"Beste RÂ²: {best_r2:.4f}\n\n")
    
    # Feature Importance hinzufÃ¼gen
    if 'feature_importance' in locals():
        f.write("TOP-FEATURES FÃœR MARKTWERT:\n")
        f.write("-" * 30 + "\n")
        for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):
            f.write(f"{i:2d}. {row['Feature']}: {row['Importance']:.4f}\n")
    
    f.write(f"\nML-FEATURES VERWENDET:\n")
    f.write("-" * 30 + "\n")
    for i, feature in enumerate(ml_features, 1):
        f.write(f"{i:2d}. {feature}\n")

print(f"   âœ… Report gespeichert: {report_file}")

# 2. ERGEBNISSE ALS CSV (fÃ¼r Excel-Import)
results_df = pd.DataFrame({
    'Modell': list(results.keys()),
    'R2_Score': [results[k]['r2'] for k in results.keys()],
    'R2_Prozent': [results[k]['r2']*100 for k in results.keys()],
    'MAE_Euro': [results[k]['mae'] for k in results.keys()],
    'RMSE_Euro': [results[k]['rmse'] for k in results.keys()]
})

csv_file = os.path.join(output_dir, f"R2_MAE_Results_{timestamp}.csv")
results_df.to_csv(csv_file, index=False, encoding='utf-8')
print(f"   âœ… CSV gespeichert: {csv_file}")

# 3. FEATURE IMPORTANCE ALS CSV
if 'feature_importance' in locals():
    feature_file = os.path.join(output_dir, f"Feature_Importance_{timestamp}.csv")
    feature_importance.to_csv(feature_file, index=False, encoding='utf-8')
    print(f"   âœ… Feature Importance gespeichert: {feature_file}")

# 4. VISUALISIERUNGEN ERSTELLEN UND SPEICHERN
print(f"\nğŸ“Š ERSTELLE VISUALISIERUNGEN...")

# Plot 1: Predicted vs Actual
plt.figure(figsize=(15, 10))

# Subplot 1: Predicted vs Actual (Bestes Modell)
plt.subplot(2, 2, 1)
best_predictions = results[best_model_name]['predictions']
plt.scatter(y_test/1000000, best_predictions/1000000, alpha=0.6, color='darkblue', s=40)
plt.plot([y_test.min()/1000000, y_test.max()/1000000], 
         [y_test.min()/1000000, y_test.max()/1000000], 'r--', lw=2)
plt.xlabel('Echter Marktwert (Mio â‚¬)', fontsize=12)
plt.ylabel('Vorhergesagter Marktwert (Mio â‚¬)', fontsize=12)
plt.title(f'Predicted vs Actual ({best_model_name})\nRÂ² = {best_r2:.3f}', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

# Subplot 2: Model Comparison
plt.subplot(2, 2, 2)
model_names = list(results.keys())
r2_scores = [results[k]['r2'] for k in model_names]
colors = ['steelblue', 'orange'] if len(model_names) == 2 else ['steelblue']

bars = plt.bar(model_names, r2_scores, color=colors)
plt.title('Model Comparison (RÂ² Score)', fontsize=14, fontweight='bold')
plt.ylabel('RÂ² Score', fontsize=12)
plt.ylim(0, max(r2_scores) * 1.1)

# Werte auf Balken anzeigen
for bar, score in zip(bars, r2_scores):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
             f'{score:.3f}', ha='center', fontweight='bold')

plt.grid(True, alpha=0.3, axis='y')

# Subplot 3: Feature Importance (Top 8)
if 'feature_importance' in locals():
    plt.subplot(2, 2, 3)
    top_features = feature_importance.head(8)
    plt.barh(range(len(top_features)), top_features['Importance'], color='green', alpha=0.7)
    plt.yticks(range(len(top_features)), top_features['Feature'])
    plt.xlabel('Importance', fontsize=12)
    plt.title('Top Features fÃ¼r Marktwert', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3, axis='x')

# Subplot 4: Residuals (Fehler-Analyse)
plt.subplot(2, 2, 4)
residuals = y_test - best_predictions
plt.scatter(best_predictions/1000000, residuals/1000000, alpha=0.6, color='green', s=40)
plt.axhline(y=0, color='r', linestyle='--', linewidth=2)
plt.xlabel('Vorhergesagter Marktwert (Mio â‚¬)', fontsize=12)
plt.ylabel('Vorhersage-Fehler (Mio â‚¬)', fontsize=12)
plt.title('Fehler-Analyse (Residuals)', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

plt.tight_layout()

# Speichere Plot
plot_file = os.path.join(output_dir, f"R2_MAE_Analysis_{timestamp}.png")
plt.savefig(plot_file, dpi=300, bbox_inches='tight')
print(f"   âœ… Visualisierung gespeichert: {plot_file}")
plt.show(block=False)

# FINALE ZUSAMMENFASSUNG
print(f"\nğŸ¯ FINALE ZUSAMMENFASSUNG")
print("=" * 60)
print(f"âœ… EVALUATION ABGESCHLOSSEN!")
print(f"   ğŸ“Š Beste Performance: {best_model_name}")
print(f"   ğŸ¯ RÂ² Score: {best_r2:.3f} ({best_r2*100:.1f}%)")
print(f"   ğŸ’° MAE: {results[best_model_name]['mae']:,.0f} â‚¬")
print(f"   ğŸ“ˆ QualitÃ¤t: ", end="")

if best_r2 >= 0.7:
    print("EXZELLENT (Publikationsreif!)")
elif best_r2 >= 0.5:
    print("SEHR GUT (Hervorragend fÃ¼r Uni-Projekt!)")
elif best_r2 >= 0.3:
    print("GUT (Solide Performance!)")
else:
    print("BASIS (Verbesserung mÃ¶glich)")

print(f"\nğŸ’¾ ALLE ERGEBNISSE GESPEICHERT IN:")
print(f"   ğŸ“ {output_dir}")
print(f"   ğŸ“„ {os.path.basename(report_file)}")
print(f"   ğŸ“Š {os.path.basename(csv_file)}")
print(f"   ğŸ“ˆ {os.path.basename(plot_file)}")

if 'feature_importance' in locals():
    print(f"   ğŸ¯ {os.path.basename(feature_file)}")

print(f"\nğŸ“ BEREIT FÃœR PRÃ„SENTATION!")
print("=" * 60) 
